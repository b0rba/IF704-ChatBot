{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6d6fa8",
   "metadata": {},
   "source": [
    "# #101\n",
    "\n",
    "1. Create an account in [Kaggle](https://www.kaggle.com/)\n",
    "2. Go to your account (https://www.kaggle.com/{username}/account)\n",
    "3. Generate a new API Token if you don't have one\n",
    "4. Upload the downloaded `kaggle.json` in this notebook folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10472b2a",
   "metadata": {},
   "source": [
    "# Installing Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a944b297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /home/borba/.local/lib/python3.8/site-packages (1.5.12)\n",
      "Requirement already satisfied: pandas in /home/borba/.local/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: nltk in /home/borba/.local/lib/python3.8/site-packages (3.6.5)\n",
      "Requirement already satisfied: tensorflow in /home/borba/.local/lib/python3.8/site-packages (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /home/borba/.local/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: python-dateutil in /usr/lib/python3/dist-packages (from kaggle) (2.7.3)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from kaggle) (2.22.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /home/borba/.local/lib/python3.8/site-packages (from kaggle) (4.62.3)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle) (2019.11.28)\n",
      "Requirement already satisfied: python-slugify in /home/borba/.local/lib/python3.8/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from kaggle) (1.25.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/borba/.local/lib/python3.8/site-packages (from pandas) (1.21.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/borba/.local/lib/python3.8/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: joblib in /home/borba/.local/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/lib/python3/dist-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (0.22.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (4.0.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/borba/.local/lib/python3.8/site-packages (from tensorflow) (1.41.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/borba/.local/lib/python3.8/site-packages (from scikit-learn) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/borba/.local/lib/python3.8/site-packages (from scikit-learn) (1.7.2)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/borba/.local/lib/python3.8/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/borba/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow) (45.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/borba/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/borba/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/borba/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/borba/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/borba/.local/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/borba/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/borba/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/borba/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/borba/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/borba/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/borba/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/borba/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle pandas nltk tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58c5612c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/borba/.kaggle’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a95a3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp kaggle/kaggle.json ~/.kaggle\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d16ac",
   "metadata": {},
   "source": [
    "# Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15f29857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading outofscope-intent-classification-dataset.zip to /home/borba/Workplace/college/IF704-ChatBot/notebooks\n",
      "100%|█████████████████████████████████████████| 285k/285k [00:00<00:00, 879kB/s]\n",
      "100%|█████████████████████████████████████████| 285k/285k [00:00<00:00, 876kB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d stefanlarson/outofscope-intent-classification-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd863f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('./dataset'):\n",
    "        os.makedirs('./dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89242d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'outofscope-intent-classification-dataset.zip': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!mv outofscope-intent-classification-dataset.zip ./dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71dee2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('./dataset/outofscope-intent-classification-dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbfe6db",
   "metadata": {},
   "source": [
    "# Setup Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70151b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/borba/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2021-12-04 08:57:25.671188: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-04 08:57:25.671225: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ignore words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# tokenize and vetorize text\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# one-hot encoding labels\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# deep learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Conv2D\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc4386ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"it's\", 'is', 'of', 'hasn', 'so', 'were', 'doesn', 'this', 'or', 'can', 'its', 'herself', 'hadn', 'them', 't', 'being', 'be', 'needn', 'won', \"mightn't\", 'same', 'you', 'm', 'wasn', \"needn't\", 'having', 'had', 'too', 'but', 'was', 'has', 'those', 'own', 'does', 'did', \"wasn't\", 'about', 'll', \"wouldn't\", 'themselves', \"weren't\", 'him', 'on', 'between', 're', 'his', 'their', 'yours', 'it', 'hers', 'who', 'any', 'if', 'from', \"don't\", 'shan', 'at', 'with', \"couldn't\", 'am', 'above', 'other', 'ourselves', 'most', 'will', 'myself', 'aren', \"shan't\", \"doesn't\", 'what', 'down', 'just', 'they', 'shouldn', \"hadn't\", 'she', 'very', 'that', 'been', 'further', 'over', 'how', 'didn', 'during', 'by', 'ma', \"isn't\", 'after', 'ours', \"didn't\", \"won't\", 'which', 'ain', \"aren't\", 'mustn', 'no', 's', 'when', 'haven', 'while', 'once', 'not', 'mightn', 'into', 'd', 'y', 'these', 'here', 'me', 'until', \"mustn't\", 'doing', \"you'll\", 'an', 'why', 'a', 'her', 'more', \"should've\", 'weren', 'then', 'in', \"hasn't\", 'and', 'up', 'are', 'o', 'he', \"she's\", 'where', 'don', \"shouldn't\", 'i', 'our', 'theirs', 'both', 'do', 'below', 'himself', 'isn', 'some', \"you'd\", 'because', 'yourselves', 'again', 'have', 'yourself', 'only', 'to', 'wouldn', 'now', 'through', \"you're\", 'whom', 'under', 'few', 've', 'for', 'there', 'each', 'against', 'than', 'the', \"that'll\", 'such', 'itself', 'couldn', 'out', 'all', \"you've\", 'off', 'my', 'as', 'your', 'before', 'nor', 'should', \"haven't\", 'we'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "words = set(stopwords.words(\"english\"))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b35c177",
   "metadata": {},
   "source": [
    "# Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "186da6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_test_json = pd.read_json(\"./dataset/is_test.json\")\n",
    "scope_train_json = pd.read_json(\"./dataset/is_train.json\")\n",
    "scope_val_json = pd.read_json(\"./dataset/is_val.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a8514f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting dialogs\n",
    "scope_test_text = [str(line).strip() for line in scope_test_json[0]]\n",
    "scope_train_text = [str(line).strip() for line in scope_train_json[0]]\n",
    "\n",
    "model_text = scope_train_text + scope_test_text\n",
    "scope_val_text = [str(line).strip() for line in scope_val_json[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bf9a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting labels\n",
    "scope_test_labels = [str(line).strip() for line in scope_test_json[1]]\n",
    "scope_train_labels = [str(line).strip() for line in scope_train_json[1]]\n",
    "\n",
    "model_labels = scope_train_labels + scope_test_labels\n",
    "scope_val_labels = [str(line).strip() for line in scope_val_json[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa2b4f",
   "metadata": {},
   "source": [
    "# Tokenize words from dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6523515",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(model_text)\n",
    "word_index = tok.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4b6f3",
   "metadata": {},
   "source": [
    "# Vectorizing dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ea1de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tokens = tok.texts_to_sequences(model_text)\n",
    "\n",
    "max_vocabulary_size = len(word_index) + 1\n",
    "input_length = max(map(lambda x: len(x), model_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf0becdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = pad_sequences(model_tokens, input_length)\n",
    "\n",
    "validation_tokens = tok.texts_to_sequences(scope_val_text)\n",
    "validation_input = pad_sequences(validation_tokens, input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a082d1",
   "metadata": {},
   "source": [
    "# One-hot encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bff8d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_transformer = preprocessing.LabelEncoder()\n",
    "label_transformer.fit(model_labels)\n",
    "\n",
    "encoded_validation_labels = label_transformer.transform(scope_val_labels)\n",
    "encoded_model_labels = label_transformer.transform(model_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64e01110",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_validation_labels = to_categorical(np.asarray(encoded_validation_labels))\n",
    "categorical_model_labels = to_categorical(np.asarray(encoded_model_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9fb35",
   "metadata": {},
   "source": [
    "# Split train data to isolate test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31bb2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(model_input, categorical_model_labels, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c05c6ec",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3d94ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    return c1 / c2\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    return c1 / c3\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1317bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import metrics\n",
    "\n",
    "model = Sequential([\n",
    "  Embedding(max_vocabulary_size, 300, input_length=input_length),                 \n",
    "  Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
    "  MaxPooling1D(pool_size=3),\n",
    "  Flatten(),\n",
    "  Dense(180, activation='relu'),\n",
    "  Dense(150, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy', precision, recall, f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "24cb077a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_16 (Embedding)    (None, 28, 300)           1787400   \n",
      "                                                                 \n",
      " conv1d_16 (Conv1D)          (None, 21, 32)            76832     \n",
      "                                                                 \n",
      " max_pooling1d_16 (MaxPoolin  (None, 7, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 224)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 180)               40500     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 150)               27150     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,931,882\n",
      "Trainable params: 1,931,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e2127735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "488/488 [==============================] - 12s 22ms/step - loss: 3.2231 - accuracy: 0.2952 - precision: 0.0909 - recall: 0.5301 - f1_score: 0.1478\n",
      "Epoch 2/6\n",
      "488/488 [==============================] - 11s 22ms/step - loss: 0.5544 - accuracy: 0.8538 - precision: 0.3165 - recall: 0.8508 - f1_score: 0.4573\n",
      "Epoch 3/6\n",
      "488/488 [==============================] - 11s 22ms/step - loss: 0.1753 - accuracy: 0.9544 - precision: 0.3920 - recall: 0.9271 - f1_score: 0.5463\n",
      "Epoch 4/6\n",
      "488/488 [==============================] - 11s 22ms/step - loss: 0.0763 - accuracy: 0.9811 - precision: 0.4169 - recall: 0.9567 - f1_score: 0.5768\n",
      "Epoch 5/6\n",
      "488/488 [==============================] - 11s 22ms/step - loss: 0.0430 - accuracy: 0.9892 - precision: 0.4421 - recall: 0.9621 - f1_score: 0.6014\n",
      "Epoch 6/6\n",
      "488/488 [==============================] - 11s 22ms/step - loss: 0.0282 - accuracy: 0.9936 - precision: 0.4220 - recall: 0.9725 - f1_score: 0.5849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbab8c88220>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9944b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 4ms/step - loss: 0.4237 - accuracy: 0.9044 - precision: 0.3592 - recall: 0.8985 - f1_score: 0.5102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42374059557914734,\n",
       " 0.904358983039856,\n",
       " 0.3591681718826294,\n",
       " 0.898455798625946,\n",
       " 0.5102159976959229]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5444d6cb",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2bdeb58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(validation_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "643c2ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8523333333333334\n"
     ]
    }
   ],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    return np.equal(np.argmax(y_true, axis=-1), np.argmax(y_pred, axis=-1)).mean()\n",
    "\n",
    "print(acc(categorical_validation_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a4be5218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent(sentence):\n",
    "  data = [[sentence]]\n",
    "  df = pd.DataFrame(data)\n",
    "  input = df[0]\n",
    "  input = tok.texts_to_sequences(input)\n",
    "  input = pad_sequences(input, input_length)\n",
    "  prediction = model.predict(input)\n",
    "\n",
    "  return model_labels[np.where(encoded_model_labels == np.argmax(prediction))[0][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e13f6f",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "704ee763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_intent(\"how can i say meet me in the bar in spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "176118d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_intent(\"translate hello word to spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a4c61973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_intent(\"how can i say posso pegar teu carro emprestado in english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939ce4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
